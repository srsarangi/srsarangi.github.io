<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head>


<meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><title>Features</title>

<meta name="keywords" content="" />
<meta name="description" content="" />
<link href="default.css" rel="stylesheet" type="text/css" media="all" />
<style type="text/css">
@import "layout.css";
</style><!--[if IE 6]><link href="ie6fix.css" rel="stylesheet" type="text/css" /><![endif]--><!--[if IE 7]><link href="ie6fix.css" rel="stylesheet" type="text/css" /><![endif]--></head><body>
<div id="wrapper">
	<div id="header-wrapper">
		<div id="header" class="container">
			<div id="logo">
				<h1>
				<a href="index.html">
					<div class="image-style1 image-style1a">
						<img src="images/logo.png" alt="" height="100" width="200" />
					</div>
				</a>
				</h1>
			</div>
			<div id="menu">
				<ul>
					<li><a href="index.html" accesskey="1" title=""><span>Home</span></a></li>
					<li class="active"><a href="features.html" accesskey="2" title=""><span>Features</span></a></li>
					<li><a href="license.html" accesskey="3" title=""><span>License</span></a></li>
					<li><a href="install.html" accesskey="4" title=""><span>Installation</span></a></li>
					<li><a href="contact.html" accesskey="6" title=""><span>Contact Us</span></a></li>
				</ul>
			</div>
		</div>
		<div id="top-header" class="container">
			<h2>Features in TEJAS</h2>
		</div>
	</div>

	<div id="bg1">
		<div id="bg2">
			<div id="bg3">
				<div id="page" class="container">
					<div id="wide-content">
						<div id="box1" class="box-style">
							<div class="content">
								<h2>SYSTEM ARCHITECTURE OF TEJAS</h2>
								<p style="text-align: center;"><img title="TEJAS Architecture" src="images/tejas_arch.jpg" alt="" height="650" width="650" /></p>
								<h3>The System Architecture of Tejas consists of the following parts:</h3>
								<ol>
									<li><a href="#PINInstrumentation">PIN Instrumentation</a></li>
									<li><a href="#CommunicationMedium">Communication Medium</a></li>
									<li><a href="#FeederEngine">Feeder Engine</a></li>
									<li><a href="#Translator">Translator</a></li>
									<li><a href="#Pipelines">Pipelines</a></li>
									<li><a href="#MemorySystem">Memory System</a></li>
									
									<li><a href="#NetworkOnChip">Network On Chip</a></li>
								</ol>
								<p>&nbsp;</p>
								<a name="PINInstrumentation"><big><big>1.</big></big><big><big> </big></big><big style="text-decoration: underline;"><big>PIN Instrumentation</big></big><br />
</a>
								<br />


								<p>CausalityTool implements the functions to communicate with
PIN tool. It initializes the PIN by giving appropriate parameters and
also contains different PIN instrumentation functions. Different
instrumentation functions that are implemented in Tejas is as follows.</p>
<p><br />
CausalityTool contains the thread start and thread finish
instrumentations functions. Thread start function put a check for the
currently active threads and also set the status of the thread to
alive. Correspondingly thread finish functions decreases the number of
alive threads. Another instrumentation function that implemented in
this tool is Instruction. This functions is getting called upon each
instruction. Within this function, we are checking for the type of the
instruction and check whether it is a branch or memory operation. We do
the respective functions for branch and memory operations. FlagRtn
function implements the instrumentation function for routines like
pthread cond broadcast.CausalityTool implements the function to handle
the finish signal from PIN side. Subset simulation functionality is
also implemented in causalityTool within printip() function. It checks
for the current number of instructions and complete the simulation once
it reach the specified limit.</p>
								<ul>
<li> Speed of causalityTool: <br />
 <img src="images/causality.png" height="266" width="834" /> </li>
</ul><br />
<a name="CommunicationMedium" /><big><big>2. </big></big><big style="text-decoration: underline;"><big>Communication Medium</big></big><br />
<br />

								
<p>All the application threads run in parallel, and potentially
generate gigabytes of data per second. It is necessary to send all of
this data to Tejas using a high throughput channel. We evaluated
several options – shared memory,memory mapped files, network sockets,
and Unix pipes. Unix pipes were found to be the slowest of all. Hence,
we ceased to consider them early in our design process. As an
experiment, we wrote a small compute intensive loop based benchmark.
For this experiment, we measured the time that it takes to transfer 1
GB of data from PIN to Java. Since we have finite buffers, we need a
method for the consumer(Java thread) to indicate to the producer(PIN)
that it cannot accept more packets for the time being. In the case<br />
of sockets, we use a dedicated socket from the consumer to the
producer, and for shared memory or memory mapped files, we use shared
variables to indicate the status of the consumer. Figure shown below
gives the results about comparison of various inter-process
communication mechanisms.</p>
<p style="text-align: center;"><img title="Comparison of various inter-process communication mechanisms" src="images/IPC.png" alt="" height="400" width="650" /></p>
<p>We observe that sockets are the slowest (10 MBps). This is because
they make costly system calls to transfer data across the processes,
and buffering is done by the kernel. However, they are also very
versatile. We can run the application threads on one machine, and run
simulator threads on another remote machine. For high throughput,
shared memory is the best option (24 MBps). Communication with memory
mappedfiles is slower, because we need to synchronize data with the
hard disk, or the disk cache in main memory.</p>
<h3>Shared Memory</h3>
<p>We use the shmget and shmat calls in Linux to get and attach shared
memory segments to the PIN processes. Since the number of segments are<br />
limited, we use a single shared memory segment. For n application
threads, we split the shared memory segment into n separate contiguous
regions. The structure is shown in the diagram below. Each region
contains a header, and a circular queue. We fix the size of each packet
to 192 bytes, and allocate space for 50 packets in each circular queue.
The header contains the status of the thread, the number of outstanding
packets in the queue, and its start and end locations.</p>

<p style="text-align: center;"><img title="Shared Memory" src="images/sharedmemory.png" alt="" height="364" width="576" /></p>

<p>The important point to note here is that the custom functions in PIN
are written in C++, and the simulator threads are written in Java.
There is no support for Linux shared memory segments in Java. Hence, we
use the Java Native Interface (JNI) to access shared memory. It allows
us to write code in C that can be linked to the JVM in runtime. The
second issue is that of locking. We need to be able to get locks to
update the pointers to the circular queue, and the count variable. We
use a Peterson lock optimized for the Intel x86 TSO (total store order)
memory model. The code shown below implements a Peterson Lock for TSO
systems with three fences (hardware and compiler).</p>
<p style="text-align: center;"><img src="images/algo.png" alt="" height="350" width="600" /></p>
<p>Along with updating the header, we also read the execution packets
that have already been transferred to Tejas using dedicated JNI
routines. JNI uses Intel x86 string instructions to transfer large
amounts of data between memory locations in one go. Hence, we
homogenized the structure of a packet, at the cost of space. Each
packet contains three 64 byte fields. We use a single JNI call,
memcpy(void∗, const void∗, size t), to copy an entire memory area into
the address space of Java threads. Secondly, note that Java is
big-endian while Intel x86 is little-endian. JNI handles this by
providing its own datatypes such as jint, and jlong. While transferring
values to the JVM, JNI routines seamlessly convert little endian values
to big endian values.</p>
<p><br />
</p>



								<p><a name="FeederEngine" />&nbsp;<big><big>3.</big></big><big><big> <span style="text-decoration: underline;">Feeder Engine</span></big></big><big style="text-decoration: underline;"><big></big></big><br />

</p>Gigabytes of data generated by the application threads has to be
received by Tejas via a feeder engine which is responsible for
transferring instruction packets to the translation engine. We use a
single JNI call, memcpy(void∗, const void∗, size t), to copy an entire
memory area into the address space of Java threads. Secondly, note that
Java is big-endian while Intel x86 is little-endian. JNI handles this
by providing its own datatypes such as jint, and jlong. While
transferring values to the JVM, JNI routines seamlessly convert little
endian values to big endian values.Several problems were faced during
the construction of the feeder engine.
<p>&nbsp;</p>
<ol>
<li> Due to the difference in speed of the production and consumption
of packets, pipelines of many cores go empty for many cycles. This
difference causes variability over several executions. This gives
instability in IPC over several executions of the same benchmark. To
counter this problem, we maintained a record of the live application
threads and kept track of the packets being received from the
communication medium. If the pipeline of any of the core goes empty,
then all the pipelines wait for that pipeline to get filled. This helps
in the stabilization of IPC as we execute any cycle when all the
pipelines have atleast one instruction to execute. </li>
<li> In case some application thread is stuck due to a synchronization
primitive, like waiting on a lock, signal or barrier, then all other
threads were getting stuck along with that thread after implementation
of case 1. This sometimes resulted in a deadlock situation, as other
threads were waiting for the stuck thread's packets to come up while
the thread which is stuck is waiting for some thread to release the
lock(or any similar case). This creates a circular waiting condition.
To counter this condition, we instrumented <em>function_enter</em> and <em>function_exit</em>
on the emulator side and send packets over the simulator side to tell
that a particular thread has entered a synchronization call and it may
get stuck. After receiving such packet, we do not count that thread as <em>live</em> till we receive a <em>function_exit</em> packet for that thread for that particular synchronization call. </li>
</ol>
<p>&nbsp;</p>
								<a name="Translator"><big><big>4.</big></big><big><big> </big></big><big style="text-decoration: underline;"><big>Translator</big></big><br />
</a>
<h4> Comparison with other simulators </h4>
<p>Typically, researchers in the field of computer architecture use
simulators like simple-scalar and super-scalar to implement their
research ideas. The challenge with these simulators is that they are
tightly coupled with an instruction set. simple scalar is based on
Portable Instruction Set Architecture (PISA) and super-scalar is based
on the MIPS instruction set. Both PISA and MIPS are not the most widely
used instruction sets. To simulate a workload for this architecture on
a standard x86 desktop machine, the user has to cross compile the
benchmark for a specific ISA. From previous experiences, it has been
found that cross-compiling standard benchmarks like Parsec and Splash2
for the MIPS and PISA instruc- tion set is tough. Even after cross
compilation, we do not retain all the architectural optimizations that
a compiler would have performed for the desired architecture
(gcc_optimizations). In Tejas, we avoid this problem completely by
accepting an executable in the instruction set in which it needs to be
simulated. We convert the instruction set into a Virtual Instruction
Set Architecture (VISA) at the loading time of the simulator. The major
problem in supporting multiple ISA is to parse them completely at the
byte level format. We relax this constraint by using standard GNU
utility like objdump to perform the first level disassembly for us. We
do not try to parse all the instructions in the executable and this
helps us to make the design of translator to be simple and scalable.</p>
<p>&nbsp;</p>
<h4> VISA (Virtual Instruction Set Architecture) </h4>
<p> The VISA instruction set is fairly abstract, and it has sufficient
information to perform a timing simulation. It is not concerned with
different behavioural aspects of the instruction set. Almost all major
architectural simulators break down emulated instructions into a
simpler instruction set.
We support four different type of operands - integer register, floating
point register, immediate value and memory operand. A memory operand
can have an integer register and an immediate value as sub-operands. We
support three different operations on integer and floating point
operands - ALU, Multiplication and Division.
Load can fetch a value from memory into an integer or floating point
register. Jump represents an unconditional transfer control to a
different point in the program, whereas branch uses a conditional
statement.</p>
<p style="text-align: center;"><img src="images/tejas_structure.jpg" height="600" width="500" /></p>
<p>The basic structure of the Tejas simulator is shown above. We
disassemble the input executable using the GNU objdump utility. The
disassembly information is fed to a translator module which takes as
input, an instruction in native low-level instruction set like x86 and
ARM, and outputs a list of instructions in VISA. We call this VISA
operations, micro operations. In order to obtain the execution trace of
the program, we run the program on the native machine and collect
useful information like load/store and thread start/stop using Pin
tool. This instrumented information is passed to Tejas using different
mechanisms like shared memory, sockets and pipes. </p>
<p>&nbsp;</p>
<p> The translator module of the simulator has to map different
instruction sets on VISA. A CISC instruction set like x86 contains
hundreds of instructions. Some instructions are rare and they do not
fall in the architecturally significant domain. It is not possible to
map all these type of instructions on VISA, we typically translate a
large number of most commonly used instructions in the given
instruction set architecture. Since, the program execution is performed
on a different emulator like Qemu or a native machine, we always have a
correct flow of the application.Thus, can afford to skip some
instructions in order to keep the design of the simulator, simple and
scalable. </p>
<p>&nbsp;</p>
<h4> Coverage of Translator </h4>
<p> The skipping of certain instructions in the executable can cause
incorrect results if we skip the commonly occurring instructions. We
have defined metrics; namely static coverage and dynamic coverage of
the translator to measure its completeness.
</p><p>&nbsp;</p>Static Coverage of the translator is the fraction of
instructions in the object executable that could be translated into
VISA. We also define the Dynamic Coverage of the simulator to the
fraction of dynamically executed instructions that were translated to
VISA. The Dynamic Coverage of the simulator will reduce if a certain
non-translated instruction was executed in a loop repeatedly. We have
noted that the coverage of the translator for i386 instruction set is
above 95 % for most of the benchmarks. The formula for dynamic coverage
does not cover complex instructions like rep. However, while comparing
the translated instructions with the execution trace, we found that the
number of such instructions was comparatively less.<br />
<br />
<br />

								<p>
								<a name="Pipelines"><big><big>5.</big></big><big><big> <span style="text-decoration: underline;">Pipelines</span></big></big><big style="text-decoration: underline;"><big></big></big><br />&nbsp;
</a><br />
</p>
<p><strong><u>Pipeline Types</u></strong></p>
<p>Tejas provides two pipeline types :</p>
<ul>
<li> Multi-Issue In-Order </li>
<li> Out-of-Order </li>
</ul>
<p><strong><u>Semi-Event-Driven Model</u></strong></p>
<p>Tejas follows a semi-event-driven model :</p>
<ul>
<li> for activities that occur regularly, with predictable latencies
(e.g, decode stage happens every cycle (unless there is some stall,
which is not frequent), and always takes the same amount of time), an
iterative approach is applied </li>
<li> for activities that occur irregularly, with unpredictable
latencies (e.g, executing a load/store), an event-driven approach is
applied. </li>
</ul>
<p><strong><u>Features Common to Both Pipelines</u></strong></p>
<p>Both pipeline types share some common features :</p>
<ul>
<li> A unified pipeline interface exposed to the rest of the system  
<ul>
<li> A queue of instructions forms the input to the pipeline. The translator feeds instructions into this queue. </li>
<li> A function that describes the pipeline's <em>one cycle operation.</em> This function is called in the principal simulation loop, where the global time is advanced by one cycle in each iteration. </li>
</ul>
</li>
<li> Branch predictor  
<ul>
<li> The different predictors provided are : Always Taken, Always Not
Taken, Bimodal, GAg, GAp, PAg, PAp, GShare, and tournament predictors. </li>
<li> New predictors can be easily added. </li>
</ul>
</li>
</ul>
<p><strong><u>In-order Pipeline</u></strong></p>
<p>A standard 5-stage pipeline is implemented. The five stages and the description of their implementation is as follows :</p>
<ul>
<li> Instruction Fetch : An instruction is fetched from the input queue
(that is filled by the translator). A request to the i-cache is made,
the address equal to the program counter of the instruction. Meanwhile,
the instruction is placed in a fixed size buffer called the
iCacheBuffer. The instruction resides here until the i-cache responds.
Once it does, the entry from the iCacheBuffer is removed, and the
instruction fetch is deemed complete. </li>
<li> Instruction Decode : Typically a 1-cycle operation. To handle
dependencies, the time when each register's value will be ready is
maintained. An instruction stays in the decode stage until both its
source operands are ready, and the corresponding functional unit is
available. Once available, depending on the type of operation, the
destination register's <em>time when ready</em> is set. For a load
instruction, whose latency cannot be determined at decode time, the
time when ready is set to infinity. Branch prediction is performed in
the decode stage -- a mis-prediction results in a penalty of further
instruction fetches being stalled for a pre-determined number of cycles
(configurable). </li>
<li> Execute Stage : An instruction occupies the execute stage until the corresponding functional unit completes. </li>
<li> Memory Stage : If a load instruction, it occupies this stage until
the memory system responds. The corresponding destination register's
time when ready is set to the current time when the load completes. </li>
<li> Write-back Stage : Simulated as a 1 cycle operation. </li>
</ul>
<p><strong><u>Multi-issue In-order Pipeline</u></strong></p>
<p>The width of the pipeline can be more than 1 (set &lt;IssueWidth&gt;
tag in the configuration file). The architecture implemented is based
on the Intel Pentium processor (Alpert et. al., 1993). Multiple
instructions can be processed by a stage in 1 cycle This processing
happens in-order, brought about by modeling the latches between stages
as circular queues of size equal to the pipeline width. All types of
hazards are strictly handled.</p>
<p><strong><u>Out-of-Order Pipeline</u></strong></p>
<p style="text-align: center;"><img src="images/pipeline.png" alt="" height="261" width="450" /><img src="images/pipeline_stages.jpg" alt="" height="313" width="350" /></p>
<p>The out-of-order pipeline is made up of the following stages :</p>
<ul>
<li> Instruction Fetch : An instruction is fetched from the input queue
(that is filled by the translator). A request to the i-cache is made,
the address equal to the program counter of the instruction. Meanwhile,
the instruction is placed in a fixed size buffer called the
iCacheBuffer. The instruction resides here until the i-cache responds.
Once it does, the entry from the iCacheBuffer is removed, and the
instruction fetch is deemed complete. </li>
<li> Instruction Decode : Once fetched, “instruction decode” is
simulated. Again, since all details are known, only timing is simulated
by advancing the clock. A Reorder Buffer entry and a Load-Store Queue
entry (if memory operation) are made at this point. Unavailability of
free entries causes the pipeline to stall till entries are available. </li>
<li> Rename : An available physical register is assigned to the
destination operand. Unavailability causes the pipeline to stall. The
physical registers corresponging to the source oeprands are dteremined,
and their availability, that is, whether or not their values are
available in the register file, is ascertained. The rename logic is
made up of a register alias table (RAT) and a list of available
registers. </li>
<li> Instruction Window Push : The next stage involves the creation of
an Instruction Window entry. Unavailability causes pipeline stalling.
The size of the Instruction Window, Reorder Buffer and Load-Store Queue
can be specified in the configuration file. </li>
<li> Instruction Select : The select logic, in every cycle, processes
the entries in the IW, looking for ready instructions. If the operands
of an instruction in the Instruction Window are available, and a
functional unit that it can execute on is available, the instruction is
issued for execution. The issue width can be set in the configuration
file. The IW entries are processed in-order -- so if more than <em>issue width</em> number of instructions are found ready, the ones that entered the window earlier are given preference. </li>
<li> Execute : The instruction stays in the execute stage until the
execution completes. Based on the type of operation, execution times
vary -- upon issue, an event signalling completion of execution is
scheduled for <em>n</em> cycles from the current time, where <em>n</em>
is the latency of the corresponding functional unit. The latency of the
functional units can be set in the configuration file. A load
instruction can be serviced through load-store forwarding, if a store
to the same address occupies an earlier position in the queue. </li>
<li> Wake-up : Once execution completes, the instructions waiting for
the result (dependent instructions) are woken up to begin execution in
the very next cycle. This feature is elaborated below. </li>
<li> Write-back : The register corresponding to the destination operand is marked ready. </li>
<li> Commit : The last stage is the commit of the instruction. First,
if the instruction was a branch, a prediction is performed. The
prediction is compared with the actual outcome. If they differ, then
the penalty for misprediction is simulated by stalling all stages of
the pipeline for a pre-specified (in the configuration file) number of
cycles. If the instruction being committed is a store, the Load-Store
Queue is intimated that it may allow the value to be written to the
memory heirarchy. </li>
</ul>
<p><strong><u>Wake-up Select Logic</u></strong></p>
<p>Wake-up Select logic allows an instruction <em>j</em> waiting for instruction <em>i</em>’s result, to begin execution in the immediately next cycle after instruction <em>i</em>
completes execution. When, instruction i completes, it wakes up all
dependent instructions. It’s result is forwarded to the dependent
instruction through the by-pass path, essentially storage associated
with the functional units.</p>
<p>The wake-up signal is modelled as an event, “Broadcast Event”, as
it’s time cannot be statically determined. In simulation, Select is
performed before the events are processed. Therefore, the broadcast
event is scheduled at the same time as the execution complete event.
Doing this would set the availablility of the operand in this cycle
(the cycle when the producing<br />
instruction completed execution). This instruction then becomes a candidate for selection in the next cycle.</p>
<p>Two cases exist that require special handling :</p>
<ul>
<li> Suppose the consuming instruction <em>j</em> was to be selected for execution in the same cycle (begins execution in the next cycle) as when instruction <em>i</em>
completed execution. This isn’t possible with the above described
solution, as it is the wake-up is performed in the same cycle as <em>i</em>’s completion. The selection can happen only in the next cycle, and the execution would thus begin in the cycle after that. </li>
<li> Suppose the consuming instruction <em>j</em> is in the rename stage when the producing instruction i completes execution. <em>j</em> looks at the register file, and deems it’s operand unavailable. The broadcast signal sent by <em>i</em> touches only the instruction window, and is thus not seen by <em>j</em>. Subsequently, <em>j</em> enters the instruction window, and <em>i</em> updates the register file. Effectively, <em>j</em> has missed the wake-up signal and thus, remains perpetually in the pipeline. </li>
</ul>
<p>To provide for these, a second broadcast event is scheduled in the clock cycle just preceding the one when <em>i</em> completes execution. This ensures the desired behavior.</p>
<p><strong><u>Simultaneous Multi-Threading (SMT)</u></strong></p>
<p>The out-of-order pipeline is capable of simultaneous multi-threading. The pipeline is provided with <em>decode width</em>
(configurable) number of input queues that the translator can populate
-- each queue corresponds to one thread. The pipeline reads from these
queues in round-robin fashion -- one queue each cycle. Each instruction
has a <em>thread id</em> field to help distinguish it when data
dependencies are handled. When one thread faces a branch
mis-prediction, all threads are stalled for the predefined penatly
period. </p>

<p>&nbsp;</p>
								<p>
								<a name="MemorySystem"><big><big>6.</big></big><big><big> <span style="text-decoration: underline;">Memory System</span></big></big><big style="text-decoration: underline;"><big></big></big><br />&nbsp;
</a><br />
</p>
<h4> Non-uniform Cache Architecture </h4>
<p>Today’s high performance processors incorporate large level-two (L2)
caches on the processor die. These sizes will continue to increase as
the bandwidth demands on the package grow, and as smaller technologies
permit more bits per mm2. In future technologies, large on-chip caches
with a single, discrete hit latency will be undesirable, due to
increasing global wire delays across the chip. Data residing in the
part of a large cache close to the processor could be accessed much
faster than data that reside physically farther from the processor.</p>
<h4>  Types of NUCA </h4>
<ul>
<li>
Static NUCA- Data are statically mapped into banks. S-NUCA caches
static mappings of data to banks and the banks have non-uniform access
times. </li>
<li>
Dynamic NUCA- The future cache access non-uniformity can be expolited
by placing frequently accessed data in closer (faster) banks and less
important–yet still cached–data in farther banks. There are three
important questions about the management of data in the cache: <ul>
<li>
mapping: how the data are mapped to the banks, and in which banks a datum can reside
</li>
<li>
search: how the set of possible locations are searched to find a line, 
</li>
<li>
movement: under what conditions the data should be migrated from one bank to another. 
</li>
</ul>
</li>
</ul>

<p>&nbsp;</p>
<h4> Cache-Core Topology </h4>
<p>Our implementation of NUCA follows tiled architecture. Each tile
contains a core, a L1 cache , a L2 cache bank and a router.The routers
are connected in a 2D-Mesh topology. L2 cache is distributed and each
bank has its own local cache controller to handle requests from
different cores and caches. In our implementation, each row in the
design layout below is a bank-set. </p>
<p style="text-align: center;"><img src="images/cachecorelayout.png" height="400" width="450" /></p>
<p>&nbsp;</p>
<h4> Policies in S-NUCA </h4>
<p>S-NUCA follows the same strategies for both placement and searching.
The requesting core directly sends the request to the statically
determined bank based on the low-order bits of tag as shown in Fig.2.
If a miss occurs at the bank, the request is immediately forwarded to
the nearest memory controller. The data block from main memory is
placed on this statically determined bank. S-NUCA does not have any
migration policy, the evicted block from the cache bank is evicted from
the LLC and written back to main memory. </p>
<p style="text-align: center;"><img src="images/banknum.png" height="100" width="300" /></p>
<p>&nbsp;</p>
<h4> Policies in D-NUCA </h4>
<p>Like in S-NUCA, in D-NUCA low-order bits of tag determines the bank
number. In addition, it is further broken down into two parts - 1)
Bank-set number 2) Bank Number within set. A data block can be placed
in any of the banks within the bank-set.</p>
<p style="text-align: center;"><img src="images/bankset.png" height="100" width="300" /></p>
<ul>
<li>
Placement - In this strategy of block placement, when a cache miss
occurs data from main memory is kept at the bank of the bank set
nearest to the core. </li>
<li>
Searching - Data request is broadcasted to all the cache banks present
in the bank-set. To restrict network traffic within the bank-set only.
A unicast request is sent to the bank within the bank-set closest to
the core. If a miss occurs, it broadcasts the requests to all other
banks in the bank-set.The steps are shown in figure 4 below. <p style="text-align: center;"><img src="images/broadcast_mechanism.png" height="400" width="450" /></p>
</li>
<li>
Migration - In order to reduce the access latency in D-NUCA searching
schemes, we have used bank-set level block migration. When a hit occurs
for a block in the bank of bank-set, the block is migrated to the bank
closer to the core (upward migration). If same block is accessed
multiple times, the data block migrates to bank closest to core. We are
not doing any downward migration in our scheme. When a block is evicted
from the bank it is not migrated to bank farther from core instead the
block is written back to the main memory. The mechanism is shown below
in figure 5. <p style="text-align: center;"><img src="images/dnucamig.png" height="400" width="400" /></p>
</li>
</ul>



								<p>&nbsp;</p>
								<p>
								<a name="NetworkOnChip"><big><big>7.</big></big><big><big> <span style="text-decoration: underline;">Network On Chip</span></big></big><big style="text-decoration: underline;"><big></big></big><br />&nbsp;
</a><br />
</p>


<p>NOC stands for Network On Chip. NOC connects together different
network elements such as cores and cache banks. Function of an NOC is
to route the messages between the connected network elements. NOC is
used when the number of elements to be connected is so large in number.
In that case the normal one-to-one interconnection method becomes a
tedious task and also it can result in increased latency and power
consumption.</p>
<p>In Tejas, the NOC implementation is generic so that it can be used
to connect any elements(cores, cache banks). “NetworkInterface.java”
contains the interface functions to access the NOC. There are two types
of implementation for NOC : one is optical and one is electrical.
Optical NOC is implemented as a package within NOC.</p>
<p>NOC has a number of parameters that can be specified through a
configuration file for simulation. It has parameters that extended from
simulation element and also it has parameters of its own. An user can
specify the number of rows and columns for the NOC. This defines the
shape of the interconnection. One other parameters is number of
buffers. It shows the number of buffer space available for messages in
a router while passing through routers. <b>NocLatencyBetweenBanks</b>
defines the number of cycles required to move a message from one router
to other. An user can change the topology of the NOC interconnection
using the variable <b>NocTopology`</b> and he can select the routing algorithm by using <b>NocRoutingAlgorithm</b>. The interconnection type can be changed using the parameter <b>NocConnection</b>. The parameters <b>NocRouterArbiter</b> and <b>TechPoint</b> is for the power calculation of NOC.<br />
Different NOC topologies that supported in Tejas are</p>
<p>• <em>Mesh<br />• Ring<br />• Bus<br />• Torus<br />• Fat Tree<br />• Omega<br />• Butterfly</em></p>
<p>Different routing algorithms that implemented in Tejas are<br />
•<em> Simple-XY<br />• West-first<br />• North-last<br />• Negative-first </em></p>
<p>Tejas can support both static and dynamic routing schemes. Static
routing scheme select the route statically while the dynamic scheme
select the less congested route from the set of available routes.</p>
<p>Tejas implements functions for creating routers, interconnect those
routers, finding the next router from source and destination values,
allocation of buffer space, deadlock avoidance by giving priority to
outgoing requests and many other basic functions for achieving the
functionalities of an NOC.</p>
<p>In electrical NOC, request can come through the router (0,0). From
there, the request is routed to the destination through intermediate
links. The path depends upon the routing algorithm and the topology of
interconnection. There can be reply messages that coming from within
the NOC system, intended to move outside. The incoming and outgoing
path may not be equal and it depends on the routing algorithm. Any
request from some arbitrary router to another is supported. Each router
is having N number of buffers and while routing one request from one
router to other, the sender checks the availability of the buffer space
in destination router and pass the request only if there is an
availability of buffer in destination router, otherwise the sender will
wait till the buffer space is available. There is a chance of dead lock
in this case, if we are not giving any priority to the outgoing
requests from the cache banks. So we have prioritized the outgoing
request, as the incoming request can move forward only when there are
two or more buffer space in the destination while the outgoing request
can go on even when there is a single buffer space in the next hop
router. Every parameter specified here in the NOC can be configured
through the input configuration file. Static route selection scheme
will be always select the same route while dynamic routing will select
a route with less traffic from a number of available routes. The NOC is
also provide the hop counter statistics as it gives the number of
requests passed from each router. In fat tree, butterfly and omega
topologies, other than the terminal nodes there are a number of
intermediate switches to pass the request from source to destination.
In NOC one request is treated as one message, that means one event that
posted to router is considered as a message. It is not divided again
into flits. The NOC interface is implemented as generic so that it can
be re-used for interconnecting the cores in future.</p>
<p>Other than the NOC described above, which simulates electrical interconnections between routers, we have implemented an <strong><u>Optical NOC </u></strong>that
captures the optical behavior of the signal communication. Optical NOC
is hierarchical in structure and having three different types of
optical buses – data bus, token bus and broadcast bus. Data bus is
meant for passing data to and from cache banks, token bus is for
passing token from one cache bank to other and broadcast bus is to
broadcast the request for data to all the cache banks in one cluster.
Access to the data bus is controlled using the token bus as the station
which needs to send the data should collect the token from the token
bus as the token passes through the token bus and keep the token with
the station still it finishes the data transfer and then release the
token back to the token bus. Two kinds of data transfer can be possible
in this Optical NOC structure – local and global. Local messages are
intended within a cluster while the global messages are the message
which pass from one cluster to another. So multiple parallel local
messages can be happen at a time, but only one global message can be at
a time active in the optical NOC.</p>
<p>A small description about the files the NOC package is given below</p>
<p>• <em>Switch.java</em> - Contains the functions for buffer.<br />
• <em>NocInterface.java</em> - Interface to the NOC.<br />
• <em>Router.java</em> - Contains the parameters of router, route computation and handleEvent function.<br />
• <em>RoutingAlgo.java</em> - Functions that implement routing algorithms.<br />
• <em>NOC.java</em> - Defines supporting topologies and functions interconnect network elements.<br />
• <em>Databus.java, <b>BroadcastBus</b>.java, <b>TokenBus</b>.java</em> - Implement Differentoptical buses.<br />
• <em>OpticalNOC.java </em>- Contains optical parameters related to optical technology.<br />
• <em>EntryPoint.java </em>- Implements the entry point to the optical NOC.</p>
<p>&nbsp;</p>
<h4>Code Structure of NOC:</h4>
<p style="text-align: center;"><img src="images/test.png" alt="test.png" height="350" width="500" /></p>

<p>&nbsp;</p>
<h4>Optical NOC Structure:</h4>
<div style="text-align: center;"><img src="images/optical.jpg" alt="optical.jpg" height="350" width="500" />

							</div>
</div>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>
</div>
<div id="footer">
	<p>Copyright (c) 2014 Indian Institute of Technology, Delhi. All rights reserved.</p>
</div>
</body></html>